======================
REINFORCEMENT LEARNING
======================

Reinforcement learning is very different to supervised and unsupervised learning. In reinforcement learning, the algorithm learns from experience and experimentation. Essentially, it learns from trial and error.

Reinforcement learning consists of several key concepts:

1. Agent is the entity being trained.
2. Environment is the “world” in which the agent interacts.
3. Actions are performed by the agent in the environment.
4. Rewards are issued to the agent for performing good actions.

Playing games
-------------
Playing games is a classic example of applied reinforcement learning.

Let’s use the game Breakout as an example. The objective of the game is to control the paddle and direct the ball to hit the bricks and make them disappear. A reinforcement learning model has no idea what the purpose of the game is, but by being rewarded for good behavior (in this case, hitting a brick with the ball) it learns over time that it should do that to maximise reward.

In this situation, the:
a. Agent is the paddle;
b. Environment is the game scenes with the bricks and boundaries;
c. Actions are the movement of the paddle; and
d. Rewards are issued by the reinforcement learning model based upon the number of bricks hit with the ball.

Traffic signaling
-----------------
Another use case for reinforcement learning is controlling and coordinating traffic signals to minimize traffic congestion.

How many times have you driven down a road filled with traffic lights and have to stop at every intersection as the lights are not coordinated? Using reinforcement learning, the model wants to maximise its total reward which is done through ensuring that the traffic signals change to keep maximum possible traffic flow.

In this use-case, the:
a. Agent is the traffic light control system;
b. Environment is the road network;
c. Actions are changing the traffic light signals (red-yellow-green); and
d. Rewards are issued by the reinforcement learning model based upon traffic flow and throughput in the road network.

Autonomous vehicles
-------------------
A final example of reinforcement learning is for self-driving, autonomous, cars.

It's obviously preferable for cars to stay on the road, not run into anything, and travel at a reasonable speed to get the passengers to their destination. A reinforcement learning model can be rewarded for doing these things and will learn over time that it can maximize rewards by doing these things.

In this case, the:
a. Agent is the car (or, more correctly, the self-driving software running on the car);
b. Environment is the roads and surrounds on which the car is driving;
c. Actions are things such as steering angle and speed; and
d. Rewards are issued by the reinforcement learning model based upon how successfully the car stays on the road and drives to the destination.

AWS DeepRacer is a 1/18th scale racing car, with the objective being to drive around a track as fast as possible. To achieve this goal, AWS DeepRacer uses reinforcement learning.

1. The agent is the AWS DeepRacer car (or, more specifically, the software running on the car);
2. The agent wants to achieve the goal of finishing laps around the track as fast as possible, so the track is the environment.
3. The agent knows about the environment through the state which is the portion of the environment known to the agent. In the case of AWS DeepRacer, it is the images being captured by the camera.
4. Once the agent knows its state in the environment, it can perform actions in the environment to help it achieve its goal. In the case of DeepRacer, this might be accelerating, braking, turning left, turning right, or going straight.
5. The agent then receives feedback in the form of a reward about how well that action contributed towards achieving its goal.
6. And all this happens within an episode. This can be thought of as a cycle of the agent performing an action in the environment (based upon the state it has observed) and then receiving feedback in the form of a reward which informs future actions it might take.




A model training algorithm is a procedure that uses data to create a model. These algorithms maximize total reward differently. Proximal Policy Optimization (PPO) explores the environment less compared to Soft Actor Critic (SAC).

What is a reward function?
The reward function is Python code that describes immediate feedback in the form of a reward or penalty to move from a given position on the track to a new position.

What is the purpose of a reward function?
The reward function encourages the vehicle to make moves along the track quickly to reach its destination.

Reinforcement learning is essentially learning by trial and error. The agent explores the environment to gather information (called exploration) and then uses that information to try and maximize its reward (called exploitation).

When training a machine learning model an algorithm is used. Algorithms are sets of instructions, essentially computer programs. Machine learning algorithms are special programs which learn from data. This algorithm then outputs a model which can be used to make future predictions.

Policies
========

A policy defines the action that the agent should take for a given state. This could conceptually be represented as a table - given a particular state, perform this action.

This is called a deterministic policy, where there is a direct relationship between state and action. This is often used when the agent has a full understanding of the environment and, given a state, always performs the same action.

Consider the classic game of rock, paper, scissors. An example of a deterministic policy is always playing rock. Eventually the other players are going to realize that you are always playing rock and then adapt their strategy to win, most likely by always playing paper. So in this situation it’s not optimal to use a deterministic policy.

So, we can alternatively use a stochastic policy. In a stochastic policy you have a range of possible actions for a state, each with a probability of being selected. When the policy is queried to return an action for a state it selects one of these actions based on the probability distribution.

This would obviously be a much better policy option for our rock, paper, scissors game as our opponents will no longer know exactly which action we will choose each time we play.

You might now be asking, with a stochastic policy how do you determine the value of being in a particular state and update the probability for the action which got us into this state? This question can also be applied to a deterministic policy; how do we pick the action to be taken for a given state?

Well, we somehow need to determine how much benefit we have derived from that choice of action. We can then update our stochastic policy and either increase or decrease the probability of that chosen action being selected again in the future, or select the specific action with the highest likelihood of future benefit as in our deterministic policy.

If you said that this is based on the reward, you are correct. However, the reward only gives us feedback on the value of the single action we just chose. To truly determine the value of that action (and resulting state) we should not only look at the current reward, but future rewards we could possibly get from being in this state.

Value function
==============

How we can determine possible future rewards from being in a certain state?
---------------------------------------------------------------------------

This is done through the value function. Think of this as looking ahead into the future and figuring out how much reward you expect to get given your current policy.

Say a car (agent) is approaching a corner. The algorithm queries the policy about what to do, and it says to accelerate hard. The algorithm then asks the value function how good it thinks that decision was - but unfortunately the results are not too good, as it’s likely the agent will go off-track in the future due to his hard acceleration into a corner. As a result, the value is low and the probabilities of that action can be adjusted to discourage selection of the action and getting into this state.

This is an example of how the value function is used to critique the policy, encouraging desirable actions while discouraging others.

We call this adjustment a policy update, and this regularly happens during training. In fact, you can even define the number of episodes that should occur before a policy update is triggered.

In practice the value function is not a known thing or a proven formula. The reinforcement learning algorithm will estimate the value function from past data and experience.

PPO and SAC
===========

PPO uses “on-policy” learning. This means it learns only from observations made by the current policy exploring the environment - using the most recent and relevant data. Say you are learning to drive a car, on-policy learning would be analogous to you reviewing a video of your most recent lesson and taking note of what you did well, and what needs improvement.

In contrast, SAC uses “off-policy” learning. This means it can use observations made from previous policies exploration of the environment - so it can also use old data. Going back to our learning to drive analogy, this would involve reviewing videos of your driving lessons from the last few weeks. Even though you have probably improved since those lessons, it can still be helpful to watch those videos in order to reinforce good and bad things. It could also include reviewing videos of other drivers to get ideas about good and bad things they might be doing.

So what are some benefits and drawbacks of each approach?

1. PPO generally needs more data as it has a reasonably narrow view of the world, since it does not consider historical data - only the data in front of it during each policy update. In contrast, SAC does consider historical data so it needs less new data for each policy update.

2. That said, PPO can produce a more stable model in the short-term as it only considers the most recent, relevant data - compared with SAC which might produce a less stable model in the short-term since it considers less relevant, historical data.

So which should you use? There is no right or wrong answer. SAC and PPO are two algorithms from a field which is constantly evolving and growing. Both have their benefits and either one could work best depending on the circumstance.

Reward Function
===============

The purpose of the reward function is to issue a reward based upon how good, or not so good, the actions performed are at reaching the ultimate goal.

In order to calculate an appropriate reward you need information about the state of the agent and perhaps even the environment.

Just because you have trained a model doesn't mean you cannot change the reward function. You might find that the model is exhibiting behavior you want to de-incentivize. In this case you may include code to penalize the agent for that behavior.

The reward function can be as simple, or as complex, as you like - just remember, a more complex reward function doesn't necessarily mean better results.


How does a Machine Learning model make decisions?
=================================================

Machine learning inference involves running input data through a machine learning model which then outputs a prediction. This prediction is based upon past experience and subsequent knowledge which the model has gained through this training. The inference process can be compute intensive.

We can measure the performance of inference with two metrics:

1. The "inference rate" which is the number of inferences which can be done per second; and
2. The "inference time" or "inference latency" which is time taken to run a single inference.

Ultimately, we want to maximize the rate of inference and therefore the number of decisions made every second. However, the rate of inference is dependent on many factors. The most obvious is the performance of the machine running the inference, such as the CPU (Central Processing Unit) speed, whether any GPU (Graphical Processing Unit) acceleration is present, and the amount of system RAM (Random Access Memory) available.

There are also other factors. One is the machine learning framework used for training. There are many different frameworks available, such as TensorFlow, PyTorch, and Apache MXNet. These frameworks provide the tools and services to build machine learning models, including the training algorithms such as PPO (Proximal Policy Optimization) and SAC (Soft Actor Critic).

Challenges in real-world applications of machine learning inference
-------------------------------------------------------------------
When people think of machine learning (ML) models they often imagine them running on large, powerful computers. Some real-world ML applications, like real-time traffic monitoring, need computers powerful enough to provide the high inference rates necessary for complex, high stakes tasks.

It also may be more cost-effective to centralize inferencing in one place. In these kinds of situations the data is fed from edge devices, such as traffic cameras, all across a city to a central server for processing.

However, what happens when sending the data to a central server for inference isn’t a good option and we need to perform inference locally?
In this situation we need to perform the inference locally on the device. We call this “inference at edge”.

However, this presents a challenge. The compute available on edge devices is restricted.
So, we need to balance the needs of efficient and effective inference while also working within the bounds of the compute available.

Addressing the challenges
-------------------------
Two places where inference can be performed: on large, powerful computers—like a cloud server—or on low-power devices. We call these low-power devices edge devices. Inference on these devices is called real-time inference at edge.

Latency is the time that it takes to receive a result back from a remote computer from the moment the data is sent out from your device. In most cases, inference at the edge results in lower latency because the input doesn’t need to be sent out—It can all be done in one place. This is particularly important to make real-time decisions. 

However, when you create a deep learning model, you have to consider that edge devices are generally less powerful and have less compute power than what is available in the cloud.

To infer data through a deep learning model we need an inference engine. If you think of the inference engine as the car’s engine then the deep learning model is the fuel. This combination determines how fast and how efficient the car will run. You want to make sure the fuel (your model) is appropriate for the engine (the device) it is running on.

Model optimization with Intel OpenVINO
--------------------------------------
OpenVINO stands for Open Visual Inferencing and Neural Network Optimization and is Intel’s developer tool kit for machine learning inference. It contains a series of components and tools which help developers improve the inference performance of their models on Intel hardware, which has a compute module with an Intel Atom processor.

There are different kinds of compute hardware available today, such as CPUs and GPUs. You can think of these types of processors as different kinds of automobiles. The CPU might be similar to a family station wagon. It’s a great all-around car that’s good for different uses, but it probably won’t win a track race. Meanwhile, the GPU is like a purpose-built four-wheel drive car. It’s great for off-roading, but isn’t easy to drive around town. OpenVINO toolkit helps your optimize machine learning models to work on different types and combinations of hardware.

The Intel OpenVINO toolkit has the following components: the Deep Learning Deployment Toolkit (includes the model optimizer), inference engine, and pre-trained models. It also includes advanced tools and libraries for expert programmers.

Using an optimized model and the OpenVINO toolkit inference engine, we can achieve a low inference latency on the Intel Atom processor.

How does the optimization process work?
---------------------------------------
There are three steps to the optimization process:

1. Convert and optimize
2. Further tuning for performance (optional)
3. Deploy the model

In the first step, we will run the model optimizer to prepare the pre-trained model for inferencing. It will generate a set of different files, known as an Intermediate Representation (IR), that the OpenVINO toolkit inference engine can understand and use. The model optimizer performs a number of optimizations on the model to make it run faster and more efficiently on the Intel Atom processor.

However, in some cases, if the performance isn’t quite what you are expecting, there are plenty of additional tools included with the OpenVINO toolkit that advanced users can use.

The Post-Training Optimization Toolkit (POT) is a tool included with the OpenVINO toolkit, which can be used to make the model more efficient. Think of this tool as a slider with accuracy at one end and performance at the other.

The POT allows you to tune a model for extra performance by reducing the precision of the model and therefore, sacrificing some accuracy. Some Intel processors come with accelerators to turbo charge performance of lower precision models and this is where the POT is particularly handy, because it is similar to putting the right fuel in a car to make sure you get the best performance from the engine.

There are also benchmarking and accuracy checker tools available, which will allow you to get key performance and accuracy data for a model on the target hardware. This data is useful as you go through the process of refining and optimizing your model to understand how it impacts inference performance and accuracy.

When you are done with any of the optional fine-tuning, the optimized model is ready to be uploaded to perform real-time inference using the OpenVINO toolkit inference engine.

